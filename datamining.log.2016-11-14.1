2016-11-14 14:24:25.978 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-14 14:24:29.597 [main] WARN   - Your hostname, andreaslicspro2.dynamic.ucsd.edu resolves to a loopback/non-reachable address: 137.110.134.186, but we couldn't find any external IP address!
2016-11-14 14:29:22.737 [Thread-2] ERROR org.apache.spark.scheduler.LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@69751ad)
2016-11-14 14:29:22.737 [Thread-2] ERROR org.apache.spark.scheduler.LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(5,1479162562737,JobFailed(org.apache.spark.SparkException: Job 5 cancelled because SparkContext was shut down))
2016-11-14 14:29:22.758 [Executor task launch worker-8] ERROR org.apache.spark.MapOutputTrackerMaster - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:110) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:212) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:152) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:166) [spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1.apply(CartesianRDD.scala:76) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1.apply(CartesianRDD.scala:75) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:413) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:350) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$class.foreach(Iterator.scala:742) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) [scala-library-2.11.7.jar:?]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) [scala-library-2.11.7.jar:?]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) [scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:308) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.to(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:300) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:287) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212) [spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212) [spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:89) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) [spark-core_2.11-1.6.2.jar:1.6.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_15]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_15]
	at java.lang.Thread.run(Thread.java:722) [?:1.7.0_15]
