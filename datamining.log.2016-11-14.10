2016-11-14 16:00:44.105 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-14 16:00:46.429 [main] WARN  org.spark-project.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_15]
	at sun.nio.ch.Net.bind(Net.java:344) ~[?:1.7.0_15]
	at sun.nio.ch.Net.bind(Net.java:336) ~[?:1.7.0_15]
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199) ~[?:1.7.0_15]
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_15]
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.server.Server.doStart(Server.java:293) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2024) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:166) [scala-library-2.11.7.jar:?]
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2015) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.Option.foreach(Option.scala:257) [scala-library-2.11.7.jar:?]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59) [spark-core_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:37) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
2016-11-14 16:00:46.435 [main] WARN  org.spark-project.jetty.util.component.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@4e1740fa: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_15]
	at sun.nio.ch.Net.bind(Net.java:344) ~[?:1.7.0_15]
	at sun.nio.ch.Net.bind(Net.java:336) ~[?:1.7.0_15]
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199) ~[?:1.7.0_15]
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_15]
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.server.Server.doStart(Server.java:293) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2024) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:166) [scala-library-2.11.7.jar:?]
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2015) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.Option.foreach(Option.scala:257) [scala-library-2.11.7.jar:?]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59) [spark-core_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:37) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
2016-11-14 16:00:46.494 [main] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2016-11-14 16:00:57.830 [main] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2016-11-14 16:00:58.003 [main] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database default, returning NoSuchObjectException
2016-11-14 16:00:58.837 [main] WARN   - Your hostname, andreaslicspro2.dynamic.ucsd.edu resolves to a loopback/non-reachable address: 137.110.134.186, but we couldn't find any external IP address!
2016-11-14 16:01:01.951 [main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204) [?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:441) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:226) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:229) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:40) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	... 61 more
Caused by: java.lang.OutOfMemoryError: PermGen space
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791) ~[?:1.7.0_15]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.7.0_15]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[?:1.7.0_15]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[?:1.7.0_15]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423) ~[?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:212) ~[spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:201) ~[spark-hive_2.11-1.6.2.jar:1.6.2]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791) ~[?:1.7.0_15]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.7.0_15]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[?:1.7.0_15]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[?:1.7.0_15]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423) ~[?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:212) ~[spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:201) ~[spark-hive_2.11-1.6.2.jar:1.6.2]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791) ~[?:1.7.0_15]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.7.0_15]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[?:1.7.0_15]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[?:1.7.0_15]
2016-11-14 16:01:02.043 [main] WARN  hive.ql.metadata.Hive - Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204) [?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:441) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:226) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:229) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:40) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at java.lang.Class.newInstance0(Class.java:374) ~[?:1.7.0_15]
	at java.lang.Class.newInstance(Class.java:327) ~[?:1.7.0_15]
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
2016-11-14 16:01:16.756 [main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204) [?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:441) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:226) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:229) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:40) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	... 58 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at java.lang.Class.newInstance0(Class.java:374) ~[?:1.7.0_15]
	at java.lang.Class.newInstance(Class.java:327) ~[?:1.7.0_15]
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	... 58 more
