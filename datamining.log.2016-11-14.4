2016-11-14 14:30:52.835 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-14 14:30:56.344 [main] WARN   - Your hostname, andreaslicspro2.dynamic.ucsd.edu resolves to a loopback/non-reachable address: 137.110.134.186, but we couldn't find any external IP address!
2016-11-14 14:31:01.177 [Executor task launch worker-7] WARN  org.apache.parquet.hadoop.ParquetRecordReader - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2016-11-14 14:32:01.844 [Thread-2] ERROR org.apache.spark.scheduler.LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@3cb0ecce)
2016-11-14 14:32:01.845 [Thread-2] ERROR org.apache.spark.scheduler.LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(8,1479162721845,JobFailed(org.apache.spark.SparkException: Job 8 cancelled because SparkContext was shut down))
2016-11-14 14:32:01.872 [Executor task launch worker-1] ERROR org.apache.spark.MapOutputTrackerMaster - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:110) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:212) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:152) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:166) [spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1.apply(CartesianRDD.scala:76) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1.apply(CartesianRDD.scala:75) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:413) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:350) [scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$class.foreach(Iterator.scala:742) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) [scala-library-2.11.7.jar:?]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) [scala-library-2.11.7.jar:?]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) [scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:308) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.to(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:300) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:287) [scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1194) [scala-library-2.11.7.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212) [spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212) [spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:89) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) [spark-core_2.11-1.6.2.jar:1.6.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_15]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_15]
	at java.lang.Thread.run(Thread.java:722) [?:1.7.0_15]
2016-11-14 14:32:01.881 [Executor task launch worker-1] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 15.0 (TID 232)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:114) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:212) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:152) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:166) ~[spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1.apply(CartesianRDD.scala:76) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1.apply(CartesianRDD.scala:75) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:413) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:350) ~[scala-library-2.11.7.jar:?]
	at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[scala-library-2.11.7.jar:?]
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) ~[scala-library-2.11.7.jar:?]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) ~[scala-library-2.11.7.jar:?]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) ~[scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:308) ~[scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.to(Iterator.scala:1194) ~[scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:300) ~[scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194) ~[scala-library-2.11.7.jar:?]
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:287) ~[scala-library-2.11.7.jar:?]
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1194) ~[scala-library-2.11.7.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212) ~[spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212) ~[spark-sql_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:89) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) [spark-core_2.11-1.6.2.jar:1.6.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_15]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_15]
	at java.lang.Thread.run(Thread.java:722) [?:1.7.0_15]
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:110) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	... 42 more
