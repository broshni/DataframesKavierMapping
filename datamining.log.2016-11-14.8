2016-11-14 15:58:15.056 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-14 15:58:28.674 [main] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2016-11-14 15:58:28.839 [main] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database default, returning NoSuchObjectException
2016-11-14 15:58:29.671 [main] WARN   - Your hostname, andreaslicspro2.dynamic.ucsd.edu resolves to a loopback/non-reachable address: 137.110.134.186, but we couldn't find any external IP address!
2016-11-14 15:58:32.864 [main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204) [?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:441) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:226) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:229) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:40) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	... 61 more
Caused by: java.lang.OutOfMemoryError: PermGen space
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791) ~[?:1.7.0_15]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.7.0_15]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[?:1.7.0_15]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[?:1.7.0_15]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423) ~[?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:212) ~[spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:201) ~[spark-hive_2.11-1.6.2.jar:1.6.2]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356) ~[?:1.7.0_15]
	at java.lang.Class.forName0(Native Method) ~[?:1.7.0_15]
	at java.lang.Class.forName(Class.java:188) ~[?:1.7.0_15]
	at org.apache.derby.impl.services.monitor.BaseMonitor.getImplementations(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.services.monitor.BaseMonitor.getDefaultImplementations(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.services.monitor.BaseMonitor.runWithState(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.impl.services.monitor.FileMonitor.<init>(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.iapi.services.monitor.Monitor.startMonitor(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.iapi.jdbc.JDBCBoot.boot(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.jdbc.EmbeddedDriver.boot(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at org.apache.derby.jdbc.EmbeddedDriver.<clinit>(Unknown Source) ~[derby-10.10.2.0.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at java.lang.Class.newInstance0(Class.java:374) ~[?:1.7.0_15]
	at java.lang.Class.newInstance(Class.java:327) ~[?:1.7.0_15]
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ~[datanucleus-rdbms-3.2.9.jar:?]
2016-11-14 15:58:32.960 [main] WARN  hive.ql.metadata.Hive - Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204) [?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:441) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:226) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:229) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:40) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at java.lang.Class.newInstance0(Class.java:374) ~[?:1.7.0_15]
	at java.lang.Class.newInstance(Class.java:327) ~[?:1.7.0_15]
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) ~[hive-exec-1.2.1.spark.jar:1.2.1.spark]
	... 21 more
2016-11-14 15:58:33.054 [main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) ~[datanucleus-api-jdo-3.2.6.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) ~[jdo-api-3.0.1.jar:3.0.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) ~[jdo-api-3.0.1.jar:3.0.1]
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) ~[jdo-api-3.0.1.jar:3.0.1]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.2.0.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) ~[hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) [hive-exec-1.2.1.spark.jar:1.2.1.spark]
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204) [?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) [?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) [?:1.7.0_15]
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:441) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:226) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:229) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101) [spark-hive_2.11-1.6.2.jar:1.6.2]
	at ORCfile.main(ORCfile.java:40) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_15]
	at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_15]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) [idea_rt.jar:?]
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	... 58 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at java.lang.Class.newInstance0(Class.java:374) ~[?:1.7.0_15]
	at java.lang.Class.newInstance(Class.java:327) ~[?:1.7.0_15]
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) ~[datanucleus-rdbms-3.2.9.jar:?]
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286) ~[datanucleus-rdbms-3.2.9.jar:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_15]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_15]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_15]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525) ~[?:1.7.0_15]
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) ~[datanucleus-core-3.2.10.jar:?]
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ~[datanucleus-api-jdo-3.2.6.jar:?]
	... 58 more
2016-11-14 15:58:34.772 [Thread-2] ERROR org.apache.spark.util.Utils - Uncaught exception in thread Thread-2
java.lang.OutOfMemoryError: PermGen space
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791) ~[?:1.7.0_15]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.7.0_15]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[?:1.7.0_15]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[?:1.7.0_15]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423) ~[?:1.7.0_15]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356) ~[?:1.7.0_15]
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1740) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239) [spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.util.Try$.apply(Try.scala:192) [scala-library-2.11.7.jar:?]
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218) [spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54) [hadoop-common-2.2.0.jar:?]
2016-11-14 15:58:34.774 [Thread-2] WARN  org.apache.hadoop.util.ShutdownHookManager - ShutdownHook '$anon$2' failed, java.lang.OutOfMemoryError: PermGen space
java.lang.OutOfMemoryError: PermGen space
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.7.0_15]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791) ~[?:1.7.0_15]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.7.0_15]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[?:1.7.0_15]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[?:1.7.0_15]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[?:1.7.0_15]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_15]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423) ~[?:1.7.0_15]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) ~[?:1.7.0_15]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356) ~[?:1.7.0_15]
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1740) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:?]
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218) ~[spark-core_2.11-1.6.2.jar:1.6.2]
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54) [hadoop-common-2.2.0.jar:?]
